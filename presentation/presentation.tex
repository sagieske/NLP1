\documentclass[t,ignorenonframetext]{beamer}
%\usepackage{beamerthemeJuanLesPins}%
%\usepackage{beamercolorthemecougar} 
%\usepackage{beamerinnerthemecircles} 
\mode<presentation>
{
  \usetheme{Darmstadt}
  \usecolortheme{rose}
  \usefonttheme{default}
}

%\usefonttheme{structuresmallcapsserif}
\usepackage{enumerate,graphicx,verbatim,url}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{algorithmic}


\newcommand\Red[1]{\textcolor{red}{#1}}

\usepackage{tikz}
\usetikzlibrary{arrows,decorations.pathmorphing,fit,positioning}

% The line below is what I talked about that makes all
% items in a list into overlays
%\beamerdefaultoverlayspecification{<+->}

\newcommand{\tc}[1]{$\backslash$\texttt{#1}}
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
  
\title{Lyrics-based Music Genre Analysis using LDA}
\author{David van Erkelens, Elise Koster \& Sharon Gieske}
\begin{document}
\frame{
\maketitle
}
\frame{
\tableofcontents
}

\section[Introduction]{Introduction}

\subsection{Motivation}
\begin{frame}
\frametitle{Motivation}
~\\
\begin{itemize}
	\item Online music services 
	%\\$\rightarrow$ personalized suggestions based on genre, collective intelligence
	\item Most music classification tasks based on audio signals 
	%\\$\rightarrow$ more storage, more noise-sensitive
	\item Earlier research did not use Topic Models for classification
\end{itemize}
~\\Applications:
\begin{itemize}
\item Automatic genre assignment
\item Finding songs with similar lyrical themes
\item Generation of lyrics
\item Modeling correlation between genres based on topic distributions
\end{itemize}
\end{frame}

\subsection{Research Question}
\begin{frame}
\frametitle{Research Question}
~\\~\\
\begin{center}
\Large{Can LDA be used to provide meaningful information about the lyrical themes in different music genres?}
\end{center}
~\\~\\
\end{frame}


\section[Approach]{Approach}
\subsection{Methods}
\frametitle{Methods}
\begin{frame}
~\\\textbf{Latent Dirichlet Allocation:} \\Topic model where each document has a distribution over topics, and each topic has a distribution over words
\begin{center}
\begin{figure}
\includegraphics[scale=0.25]{plate}
\caption{Plate diagram}
\end{figure}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Approach}~\\
\begin{enumerate}
	\item Gather dataset using crawler
	\item Preprocessing (removal of stop words etc)
	\item Gibbs sampling: sample $(topic,word)$ given $p(topic|word)$ and $p(word|topic)$
%	\item Every genre now has a distribution over topics, and every topic has a distribution over words
	\item Train SVM on array of topic-probabilities per document (and its genre)
	\item Generate `song' of length $n$ for genre $g$: select topic $k \sim \theta_g$ then select word $w \sim \varphi_k$, repeat $n$ times
\end{enumerate}

\end{frame}



\section[Results]{Results}
\subsection{Preliminary Results}
\begin{frame}~\\
\textbf{Topic Modelling}\\
Top topic (defined by its top 20 words) for some genres (after 20 iterations):

\begin{tabular} {|c|l|}
	\hline
	Religious & \specialcell{'lord', 'god', 'praise', 'jesus', 'holy', 'every', 'verse', \\ 'chorus', 'yes', 'glory', 'grace', 'power', 'know', \\'worthy', 'great', 'worship', \\'hallelujah', 'let', 'things', 'spirit'}\\
	\hline
	Holiday & \specialcell{'la', 'star', 'two', 'wish', 'like', 'bring', 'see', 'even', \\'good', 'never', 'new', 'get', 'something', 'moving',\\ 'let', 'sound', 'keep', 'christmas', 'come', 'little'} \\
	\hline 
	Rap & \specialcell{'nigga', 'bh', 'yo', 'got', 'st', 'niz', 'lil', 'get', 'fuck', \\'like', 'shit', 'fuckin', 'hoes', 'homie', 'niggas', \\ 'bang', 'ay', 'fresh', 'friends', 'first'}\\
	\hline
	\end{tabular}
\end{frame}


\begin{frame}
\textbf{Classification}
\begin{itemize}
	\item Baseline classifier: SVM trained on word counts $\sim 47 \%$ correctly classified.
	\item \begin{tabular}{|c|c|}
		\hline
		Correctly classified &  3154\\
		\hline
		Incorrectly classified & 3574\\
		\hline
		\end{tabular}

\end{itemize}

~\\~\\
\textbf{Plans for rest of project}
\begin{itemize}
	\item Test classification on topic distributions
	\item Build generator
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Challenges and lessons}~\\
\begin{itemize}
	\item Non-standard topic requires creating own crawler \& preprocessing
	\item Due to many loops, inefficient programming will cost you
	%If you re-compute all word counts every time you go over one word in one document, you're gonna have a (long) bad time. (one iteration of Gibbs sampling took more than 3 days..)
	%\item Sometimes you think you've improved something, but in reality you just broke it (results went from sensible $\rightarrow$ meaningless, turns out we just printed the wrong things)
	%\item LDA is quite a cool algorithm, if you implement it correctly
	\item `Love' is quite a common word in many music genres. So are the words `chorus', and `verse', but that may have to do with our data pre-processing.
\end{itemize}
\end{frame}

\section{Conclusion}
\subsection{Conclusion}
\begin{frame}
\frametitle{Conclusions}~\\~\\
\begin{itemize}
\setlength{\itemsep}{10pt}\setlength{\itemsep}{5pt}
\item LDA with Gibbs sampling assigns sensical topics to documents
\item Algorithms take quite some time

\end{itemize}
\end{frame}

\section[Questions]{Questions}
\begin{frame}
~ \\~ \\~ \\ ~ \\~ \\
\begin{center}\Huge Questions? \end{center} 
\end{frame}


\end{document}