\documentclass[12pt,a4paper]{amsart}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{url}
\usepackage{cite}
%\usepackage{natbib}
\usepackage{colortbl}
\usepackage{tikz}
\usetikzlibrary{arrows,decorations.pathmorphing,fit,positioning}
\usepackage{algorithmic}
\usepackage[margin=1in]{geometry}

\title{Graphical model :\\ Lyric-based genre classification}
\author{David van Erkelens (10264019)\\ Sharon Gieske (6167667)\\ Elise Koster (5982448)}

\usepackage{fancyhdr}
\usepackage{booktabs}


% Set par indents to 0
\setlength{\parindent}{0cm}

% Set header for every page (except for first page)
\pagestyle{fancy}
\rhead[\small\textsc{David van Erkelens, Sharon Gieske, Elise Koster}]{\small\textsc{Proposal NLP Mini-project}}
\lhead{\thepage.}
\cfoot{}
\date{}

\begin{document}
\maketitle
\section{Topic distribution over genres using LDA}
To model topic distributions over genres, the dataset is split into the individual genres. Then, for each genre topic distributions per document are averaged, to provide a final topic distribution over genres.

\section{Plate Diagram}


\begin{figure}[htp]
  \centering
  \begin{tikzpicture}
    [
      observed/.style={minimum size=15pt,circle,draw=gray!80,fill=gray!20},
      unobserved/.style={minimum size=15pt,circle,draw},
      hyper/.style={minimum size=1pt,circle,fill=black},
      post/.style={->,>=stealth',semithick},
    ]

    \node (w-j) [observed] at (0,0) {$w_{m,n}$};
    
    \node (z-j) [unobserved] at (0,2) {$z_{m,n}$};

    \node (y) [observed] at (0,4) {$y_n$};
    
    \node (z-prior) [unobserved] at (4,2) {$\theta_g$};
    
    \node (w-prior) [unobserved] at (-4,0) {$\varphi_k$};
    
    \node (z-hyper) [label=below:$\alpha$] at (4,0) {};
    
    \filldraw [black] (4,0) circle (3pt);
    
    \node (w-hyper) [label=above:$\beta$] at (-4,3) {};
    
    \filldraw [black] (-4,3) circle (3pt);
    
    \path
    (z-j) edge [post] (w-j)
    
    (z-hyper) edge [post] (z-prior)
    (z-prior) edge [post] (z-j)
    (y) edge [post] (z-j)


    (w-hyper) edge [post] (w-prior)
    
    (w-prior) edge [post] (w-j)
    ;

    \node [draw,fit=(w-j) (y), inner sep=30pt] (plate-context) {};
    \node [above left] at (plate-context.south east) {$N$};
    \node [draw, fit=(w-prior), inner sep=20pt] (plate-prior) {};
    \node [above left] at (plate-prior.south east) {$K$};
    \node [draw,fit=(w-j) (z-j), inner sep=15pt] (plate-token) {};
    \node [above left] at (plate-token.south east) {$M$};
    \node [draw, fit=(z-prior), inner sep=20pt] (plate-z-prior) {};
    \node [above left] at (plate-z-prior.south east) {$G$};

  \end{tikzpicture}
  \caption{Graphical model}
  \label{fig:graphical-model}
\end{figure}
\clearpage
\begin{table}[h]
\begin{tabular}{l l}
\tiny\textsc{SYMBOL} & \tiny\textsc{DESCRIPTION}\\
\hline
$K$ & \# of topics\\
$G$ & \# of genres\\
$N$ & \# of documents\\
$M_i$ & \# of words in document i\\
$y_i$ & genre of document i\\
$w_{i,j}$ & word in document i at position j\\
$z_{i,j}$ & topic assignment for word $w_{i,j}$\\
$\theta_g$ & probabilities of topics given genre g\\
$\varphi_k $ & probability of words given topic k\\
$\alpha$ & Dirichlet prior for genre distribution\\
$\beta$ & Dirichlet prior for topic distribution
\end{tabular}
\end{table}
\section{Derivation}
\begin{align}
P(w, z, \varphi, \theta | \alpha, \beta, y) &= P(\varphi | \beta)P(\theta | \alpha)P(z | \theta, y)P(w | \varphi, z)
\end{align}
\\
Separate probabilities:
\begin{align}
P(\varphi | \beta) &= \prod\limits_{k=1}^K P(\varphi_k|\beta)\\
P(\theta | \alpha) &= \prod\limits_{g=1}^G P(\theta_g|\alpha)\\
P(z | \theta, y) &= \prod\limits_{i=1}^N \prod\limits_{j=1}^{M_i} P(z_{i,j}|\theta_{y_i})\\
P(w | \varphi, z) &= \prod\limits_{i=1}^N \prod\limits_{j=1}^{M_i} P(w_{i,j}|\varphi_{z_{i,j}})
\end{align}
Joined together:
\begin{align}
P(w, z, \varphi, \theta | \alpha, \beta, y) &= \prod\limits_{k=1}^K P(\varphi_k|\beta) \prod\limits_{g=1}^G P(\theta_g|\alpha) \prod^N_{i=1}\prod^{M_i}_{j=1} P(z_{i,j}|\theta_{y_i}) P(w_{i,j}|\varphi_{z_{i,j}})
\end{align}
Integrate over joint:
\begin{align}
\int \int P(w, z, \varphi, \theta | \alpha, \beta, y) d\varphi d\theta &= \int \prod\limits_{k=1}^K P(\varphi_k|\beta) \prod^N_{i=1}\prod^{M_i}_{j=1} P(w_{i,j}|\varphi_{z_{i,j}}) d\varphi\\
&\times \int \prod\limits_{g=1}^G P(\theta_g|\alpha) \prod^N_{i=1}\prod^{M_i}_{j=1} P(z_{i,j}|\theta_{y_i}) d\theta
\end{align}
\subsection{Derivation of $\int \prod\limits_{k=1}^K P(\varphi_k|\beta) \prod^N_{i=1}\prod^{M_i}_{j=1} P(w_{i,j}|\varphi_{z_{i,j}}) d\varphi$}
~\\Use $C(k,w)$ as the number of times word $w$ is assigned to topic $k$ in any document.
\begin{align}
\prod\limits_{k=1}^K P(\varphi_k|\beta) \prod^N_{i=1}\prod^{M_i}_{j=1} P(w_{i,j}|\varphi_{z_{i,j}}) d\varphi
\end{align}
\begin{align}
&= \prod\limits_{k=1}^K \int\frac{\Gamma (\sum\limits_{w=1}^V \beta)}{\prod\limits_{w=1}^V \Gamma(\beta)} \prod\limits_{w=1}^V  \varphi_{k}(w)^{\beta-1}  \prod^N_{i=1}\prod^{M_i}_{j=1} \varphi_{z_{i,j}}(w_{i,j})) d\varphi\\
&= \prod\limits_{k=1}^K \int \frac{\Gamma (V \beta)}{(\Gamma(\beta))^V} \prod\limits_{w=1}^V  \varphi_{k}(w)^{\beta-1}  \prod\limits_{w=1}^V \varphi_{k}(w)^{C(k,w)} d\varphi\\
&= \prod\limits_{k=1}^K \int \frac{\Gamma (V \beta)}{(\Gamma(\beta))^V}  \prod\limits_{w=1}^V \varphi_{k}(w)^{\beta + C(k,w) -1} d\varphi\\
&= \prod\limits_{k=1}^K \frac{\Gamma (V \beta)}{(\Gamma(\beta))^V}  \frac{\prod\limits_{w=1}^V \Gamma(C(k,w)-1)}{\Gamma(\sum\limits_{w=1}^V C(k,w)-1))} \int \frac{\Gamma(\sum\limits_{w=1}^V C(k,w)-1))}{\prod\limits_{w=1}^V \Gamma(C(k,w)-1}  \prod\limits_{w=1}^V \varphi_{k}(w)^{\beta + C(k,w) -1} d\varphi\\
&=  \prod\limits_{k=1}^K \frac{\Gamma (V \beta)}{(\Gamma(\beta))^V}  \frac{\prod\limits_{w=1}^V \Gamma(C(k,w)-1)}{\Gamma(\sum\limits_{w=1}^V C(k,w)-1))}\\
&\propto \prod\limits_{k=1}^K \frac{\prod\limits_{w=1}^V \Gamma(C(k,w)-1)}{\Gamma(\sum\limits_{w=1}^V C(k,w)-1))}
\end{align}
\subsection{Derivation of $\prod\limits_{g=1}^G P(\theta_g|\alpha) \prod^N_{i=1}\prod^{M_i}_{j=1} P(z_{i,j}|\theta_{y_i}) d\theta$}
~\\Use $C(g,k)$ as the number of times topic $k$ is assigned to genre $g$ in any document.
\begin{align}
\prod\limits_{g=1}^G P(\theta_g|\alpha) \prod^N_{i=1}\prod^{M_i}_{j=1} P(z_{i,j}|\theta_{y_i}) d\theta
\end{align}
\begin{align}
&= \prod\limits_{g=1}^G \int\frac{\Gamma (\sum\limits_{k=1}^K \alpha)}{\prod\limits_{k=1}^K \Gamma(\alpha)} \prod\limits_{k=1}^K  \theta_{g}(k)^{\alpha-1}  \prod^N_{i=1}\prod^{M_i}_{j=1} \theta_{y_{i}}(k_{i,j})) d\theta\\
&= \prod\limits_{g=1}^G \int \frac{\Gamma (K \alpha)}{(\Gamma(\alpha))^K} \prod\limits_{k=1}^K  \theta_{g}(k)^{\alpha-1}  \prod\limits_{k=1}^K \theta_g(k)^{C(g,k)} d\theta\\
&= \prod\limits_{g=1}^G \int \frac{\Gamma (K \alpha)}{(\Gamma(\alpha))^K}  \prod\limits_{k=1}^K \varphi_{k}(w)^{\alpha + C(g,k) -1} d\theta\\
&= \prod\limits_{g=1}^G  \frac{\Gamma (K \alpha)}{(\Gamma(\alpha))^K}  \frac{\prod\limits_{k=1}^K \Gamma(C(g,k)-1)}{\Gamma(\sum\limits_{k=1}^K C(g,k)-1))} \int \frac{\Gamma(\sum\limits_{k=1}^K C(g,k)-1))}{\prod\limits_{k=1}^K \Gamma(C(g,k)-1}  \prod\limits_{k=1}^K \varphi_{k}(w)^{\alpha + C(g,k) -1} d\theta\\
&=  \prod\limits_{g=1}^G  \frac{\Gamma (K \alpha)}{(\Gamma(\alpha))^K}  \frac{\prod\limits_{k=1}^K \Gamma(C(g,k)-1)}{\Gamma(\sum\limits_{k=1}^K C(g,k)-1))} \\
&\propto  \prod\limits_{g=1}^G   \frac{\prod\limits_{k=1}^K \Gamma(C(g,k)-1)}{\Gamma(\sum\limits_{k=1}^K C(g,k)-1))} 
\end{align}

\subsection{}
~\\ Use $\neg w_{i,j}$ as collection that not uses word from document i at position j. Use $\widetilde{C}$ as a count that does not include word from document i at position j.
\begin{align}
P(z_{i,j} = k | Z_{\neg w_{i,j}}, \alpha, \beta, W) &\propto \frac{\beta + \widetilde{C}(k, w_{i,j})}{V\beta + \widetilde{C}(k)} \times \frac{\alpha + \widetilde{C}(g, k)}{K\alpha + \widetilde{C}(g)}
\end{align}
\bibliography{../papers/bibliography}
\bibliographystyle{plain}

\end{document}